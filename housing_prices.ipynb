{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1= pd.read_csv('Property_Valuation_and_Assessment_Data.csv')\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our training set:  1460 houses and 81 features\n",
      "The shape of our testing set:  1459 houses and 80 features\n",
      "The testing set has 1 feature less than the training set, which is SalePrice, the target to predict  \n"
     ]
    }
   ],
   "source": [
    "print('The shape of our training set: ',df_train.shape[0], 'houses', 'and', df_train.shape[1], 'features')\n",
    "print('The shape of our testing set: ',df_test.shape[0], 'houses', 'and', df_test.shape[1], 'features')\n",
    "print('The testing set has 1 feature less than the training set, which is SalePrice, the target to predict  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### identify categorical and numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have : 43 categorical features with missing values\n",
      "We have : 38 numerical features with missing values\n"
     ]
    }
   ],
   "source": [
    "cat=df_train.select_dtypes(include='object')\n",
    "num=df_train.select_dtypes(exclude='object')\n",
    "print('We have :',cat.shape[1],'categorical features with missing values')\n",
    "print('We have :',num.shape[1],'numerical features with missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But I know some of the features that have numeric values are categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MSSubClass',\n",
       " 'OverallQual',\n",
       " 'OverallCond',\n",
       " 'BsmtFullBath',\n",
       " 'BsmtHalfBath',\n",
       " 'FullBath',\n",
       " 'HalfBath',\n",
       " 'BedroomAbvGr',\n",
       " 'KitchenAbvGr',\n",
       " 'TotRmsAbvGrd',\n",
       " 'Fireplaces',\n",
       " 'GarageCars',\n",
       " 'PoolArea',\n",
       " 'MoSold',\n",
       " 'YrSold']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_categorical(df):\n",
    "    #function to get catergorical varaiables from my 'numeric' dataframe because some variables are numeric but categorical\n",
    "    cat_num=[]\n",
    "    for i in df.columns:\n",
    "        if df[i].unique().size<17:\n",
    "            cat_num.append(i)\n",
    "    return cat_num\n",
    "get_categorical(num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i'll keep track of these features and when its time for encoding before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['MoSold']= df_train['MoSold'].astype(str)\n",
    "# df_train['OverallQual']=df_train['OverallQual'].astype(str)\n",
    "# df_train['OverallCond']=df_train['OverallCond'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After careful studying of the features (through EDA ) decided that these at the features that should be categorical \n",
    "for the others are ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['MoSold'] = df_train['MoSold'].astype(str)\n",
    "df_train['MSSubClass'] = df_train['MSSubClass'].apply(str)\n",
    "df_train['YrSold'] =df_train['YrSold'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2\n",
       "1        5\n",
       "2        9\n",
       "3        2\n",
       "4       12\n",
       "        ..\n",
       "1455     8\n",
       "1456     2\n",
       "1457     5\n",
       "1458     4\n",
       "1459     6\n",
       "Name: MoSold, Length: 1460, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.MoSold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#missing data in Traing examples\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A this point i'll idendify the columns that have 80% of its values missing (N/A)  and drop them, also take note of them for i'lll have to drop them in the test set,  aslo for some of  the columns with less than 20% of missing data i'll decide what to do on a case to case basis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to identify and visualize the features with missing data\n",
    "def msv1(data, thresh=20, color='black', edgecolor='black', width=15, height=3): \n",
    "    \"\"\"\n",
    "    SOURCE: https://www.kaggle.com/amiiiney/price-prediction-regularization-stacking\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(width,height))\n",
    "    percentage=(data.isnull().mean())*100\n",
    "    percentage.sort_values(ascending=False).plot.bar(color=color, edgecolor=edgecolor)\n",
    "    plt.axhline(y=thresh, color='r', linestyle='-')\n",
    "    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n",
    "    plt.text(len(data.isnull().sum()/len(data))/1.7, thresh+12.5, 'Columns with more than %s%s missing values' %(thresh, '%'), fontsize=12, color='crimson',\n",
    "         ha='left' ,va='top')\n",
    "    plt.text(len(data.isnull().sum()/len(data))/1.7, thresh - 5, 'Columns with less than %s%s missing values' %(thresh, '%'), fontsize=12, color='green',\n",
    "         ha='left' ,va='top')\n",
    "    plt.xlabel('Columns', size=15, weight='bold')\n",
    "    plt.ylabel('Missing values percentage')\n",
    "    plt.yticks(weight ='bold')\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msv1(df_train, 80, color=('silver', 'gold', 'lightgreen', 'skyblue', 'lightpink'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop columns (features ) with > 80% missing vales\n",
    "c=df_train.dropna(thresh=len(df_train)*0.8, axis=1)\n",
    "print('I dropped ',df_train.shape[1]-c.shape[1], ' features in the train set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed=[]\n",
    "for i in df_train.columns:\n",
    "    if i in c.columns:\n",
    "        continue\n",
    "    else:\n",
    "        removed.append(i) \n",
    "removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allna = (c.isnull().sum() / len(c))*100\n",
    "allna = allna.drop(allna[allna == 0].index).sort_values()\n",
    "\n",
    "def msv2(data, width=12, height=8, color=('silver', 'gold','lightgreen','skyblue','lightpink'), edgecolor='black'):\n",
    "    \"\"\"\n",
    "    SOURCE: https://www.kaggle.com/amiiiney/price-prediction-regularization-stacking\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "    allna = (data.isnull().sum() / len(data))*100\n",
    "    tightout= 0.008*max(allna)\n",
    "    allna = allna.drop(allna[allna == 0].index).sort_values().reset_index()\n",
    "    mn= ax.barh(allna.iloc[:,0], allna.iloc[:,1], color=color, edgecolor=edgecolor)\n",
    "    ax.set_title('Missing values percentage per column', fontsize=15, weight='bold' )\n",
    "    ax.set_xlabel('Percentage', weight='bold', size=15)\n",
    "    ax.set_ylabel('Features ', weight='bold')\n",
    "    plt.yticks(weight='bold')\n",
    "    plt.xticks(weight='bold')\n",
    "    for i in ax.patches:\n",
    "        ax.text(i.get_width()+ tightout, i.get_y()+0.1, str(round((i.get_width()), 2))+'%',\n",
    "            fontsize=10, fontweight='bold', color='grey')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msv2(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for all the NAs assign to a varriable NA\n",
    "NA=c[allna.index.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NAcat=NA.select_dtypes(include='object')\n",
    "NAnum=NA.select_dtypes(exclude='object')\n",
    "print('We have :',NAcat.shape[1],'categorical features with missing values')\n",
    "print('We have :',NAnum.shape[1],'numerical features with missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAnum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NANUM= NAnum.isnull().sum().to_frame().sort_values(by=[0]).T\n",
    "cm = sns.light_palette(\"lime\", as_cmap=True)\n",
    "\n",
    "NANUM = NANUM.style.background_gradient(cmap=cm)\n",
    "NANUM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MasVnrArea: Masonry veneer area in square feet, the missing data means no veneer so we fill with 0\n",
    "c['MasVnrArea']=c.MasVnrArea.fillna(0)\n",
    "#LotFrontage has 16% missing values. We fill with the median\n",
    "c['LotFrontage']=c.LotFrontage.fillna(c.LotFrontage.median())\n",
    "#GarageYrBlt:  Year garage was built, we fill the gaps with the median: 1980\n",
    "c['GarageYrBlt']=c[\"GarageYrBlt\"].fillna(1980)\n",
    "#For the rest of the columns: Bathroom, half bathroom, basement related columns and garage related columns:\n",
    "#We will fill with 0s because they just mean that the hosue doesn't have a basement, bathrooms or a garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb=c[allna.index.to_list()]\n",
    "nan=bb.select_dtypes(exclude='object')\n",
    "N= nan.isnull().sum().to_frame().sort_values(by=[0]).T\n",
    "cm = sns.light_palette(\"lime\", as_cmap=True)\n",
    "\n",
    "N= N.style.background_gradient(cmap=cm)\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAcat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of missing values per column in Categorical features after the drop missing values with > 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAcat1= NAcat.isnull().sum().to_frame().sort_values(by=[0]).T\n",
    "cm = sns.light_palette(\"lime\", as_cmap=True)\n",
    "\n",
    "NAcat1 = NAcat1.style.background_gradient(cmap=cm)\n",
    "NAcat1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fill_cols = ['Electrical', 'SaleType', 'KitchenQual', 'Exterior1st',\n",
    "             'Exterior2nd', 'Functional', 'Utilities', 'MSZoning']\n",
    "\n",
    "for col in c[fill_cols]:\n",
    "    c[col] = c[col].fillna(method='ffill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will just 'None' in categorical features\n",
    "#Categorical missing values\n",
    "NAcols=c.columns\n",
    "for col in NAcols:\n",
    "    if c[col].dtype == \"object\":\n",
    "        c[col] = c[col].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will just fill 0s in the numerical features \n",
    "#Numerical missing values\n",
    "for col in NAcols:\n",
    "    if c[col].dtype != \"object\":\n",
    "        c[col]= c[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FillNA=c[allna.index.to_list()]\n",
    "\n",
    "\n",
    "\n",
    "FillNAcat=FillNA.select_dtypes(include='object')\n",
    "\n",
    "FC= FillNAcat.isnull().sum().to_frame().sort_values(by=[0]).T\n",
    "cm = sns.light_palette(\"lime\", as_cmap=True)\n",
    "\n",
    "FC= FC.style.background_gradient(cmap=cm)\n",
    "FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = c['SalePrice'].hist(bins=50)\n",
    "# ax.set_ylabel('Number of Passengers')\n",
    "plt.xticks(np.arange(0,800000,50000),rotation=45,fontsize=10)\n",
    "ax.set_xlabel('SalePrice(Dollars)',fontsize=10)\n",
    "ax.set_ylabel('Number of Listings',fontsize=12)\n",
    "ax.set_title('Sale Price Distribution',fontsize=12)\n",
    "pd.DataFrame(c['SalePrice'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c.boxplot(column= 'SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the brief description of the whole dataset, we can see in the 1460 houses, the mean value of the sale prices is 180921 USD, while the cheapest one is only 34900 USD and the most expensive one is 757000 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created an age feature TrueYear saying that considering if a building is remmodeled, the TrueYear feature is taken as the actual year of construction we would represent a more acurate relationship between the SalePrice and Yearbuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # created age column which is how old the building is\n",
    "# df['Age'] = df['YrSold'] - df['YearBuilt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #year since remodeled \n",
    "# df['SinceRemod'] = df['YrSold'] - df['YearRemodAdd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true age is since when last remodel else sice when built\n",
    "df['TrueYear'] = np.where(df['YearRemodAdd'] > 0, df['YearRemodAdd'], df['YearBuilt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Area and Bathroom  features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a feature total area an addition of all the measurements of the building  and also a bathroom an addition of the bathroom feaures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create feature total area an addition of all the measurements of the building \n",
    "df['TotalArea'] =df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GrLivArea'] +df['GarageArea']\n",
    "\n",
    "# df['Bathrooms'] = df['FullBath'] + df['HalfBath']*0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corrwith(df['SalePrice']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop target varribale\n",
    "\n",
    "df_no_SP = df.drop('SalePrice',axis=1)\n",
    "corr_all = abs(df_no_SP.corr())\n",
    "\n",
    "\n",
    "# Thresholding the correlations larger than 0.7 (or-0.7) \n",
    "corr_all_true = corr_all>0.7 \n",
    "corr_all_true = corr_all_true\n",
    "\n",
    "corr_dict = {}\n",
    "for column in corr_all_true:\n",
    "    corr_list = corr_all_true[column][corr_all_true[column]==True].index.tolist()\n",
    "    corr_dict[column] = corr_list\n",
    "\n",
    "# pprint.pprint(corr_dict)\n",
    "\n",
    "# create custom color map\n",
    "cmap = sns.color_palette(\"Reds\")\n",
    "fig1 = plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "# Generate a mask for the upper triangle\n",
    "sns.set(font_scale=0.7)\n",
    "mask = np.zeros_like(corr_all, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corr_all, mask=mask, cmap=cmap,\n",
    "          xticklabels=corr_all.columns.values,\n",
    "          yticklabels=corr_all.columns.values)\n",
    "plt.title('Pairwise Pearson Correlation of all the numeric features of houses',fontsize=11)\n",
    "\n",
    "\n",
    "# fig2 = plt.figure()\n",
    "plt.subplot(1,2,2)\n",
    "sns.heatmap(corr_all_true,mask=mask,cmap=cmap,\n",
    "           xticklabels=corr_all_true.columns.values,\n",
    "           yticklabels=corr_all_true.columns.values,\n",
    "           vmin=0.01,vmax=1)\n",
    "\n",
    "plt.title('Column pairs with correlation coefficient higher than 0.7',fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a list of all the numeric column names for our dataframe, except SalePrice\n",
    "num_cols_all = list(corr_all.columns)\n",
    "# print(cols)\n",
    "cols = num_cols_all\n",
    "\n",
    "# See how many columns there are at first\n",
    "len_1=len(num_cols_all)\n",
    "removed_features=[]\n",
    "# In the correlation dictionary, if the key is still in the columns_list, remove the items in the value from the columns_list that is not equal\n",
    "# to the key itself.\n",
    "for key, value in corr_dict.items():\n",
    "    if key in cols:\n",
    "        for feature in value:\n",
    "            if (feature in cols) & (feature != key and feature != 'TrueYear'):#keep True age\n",
    "                cols.remove(feature)\n",
    "                removed_features.append(feature)\n",
    "# Take a look how many features (columns) are removed from the last step.\n",
    "len_2 = len(cols)\n",
    "\n",
    "df = df.drop(removed_features,axis=1)\n",
    "print('There are',len(removed_features),'features removed for high pairwise correlation:\\n\\n',removed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run pairwise pearson correlation plot to confirm we have removed all the features with high pair-correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select the dependent columns from previous DataFrame with no SalePrice, calculate the cross correlation, setup same threshold\n",
    "# as before, and plot the heat map for the ture-false map. \n",
    "corr_no_repeat = abs(df[cols].corr())>0.6\n",
    "# corr_no_repeat = abs(df[cols].corr())\n",
    "fig3 = plt.figure(figsize=(8,6))\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 10}\n",
    "cmap = sns.color_palette(\"Reds\")\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr_no_repeat, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corr_no_repeat, mask=mask,cmap=cmap,\n",
    "          xticklabels=corr_no_repeat.columns.values,\n",
    "          yticklabels=corr_no_repeat.columns.values,\n",
    "          vmin=0.01,vmax=1)\n",
    "sns.set(font_scale=1.3)\n",
    "plt.title('Pairwise Pearson Correlation of Remaining Numeric Features of Houses',fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study the correlation between the top related numerical features and SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rebuild the numeric dataframe with SalePrice\n",
    "num_cols = cols\n",
    "num_df = df[num_cols]\n",
    "\n",
    "pearson_dict={}\n",
    "for col in num_df.columns:\n",
    "    pearson_dict[col] = stats.pearsonr(df[col], df['SalePrice'])\n",
    "# pprint.pprint(pear_dict)\n",
    "pearson_df = pd.DataFrame(pearson_dict,index=['correlation','p-value'])\n",
    "pearson_df=pearson_df.T.abs().sort_values(['correlation','p-value'],ascending=[False,True])\n",
    "pearson_df.plot.bar(figsize=(10,5),fontsize=13,width=1)\n",
    "plt.title('Pearson Correlation Scores and P-Values of All Numeric Features ',fontsize=16)\n",
    "plt.legend(fontsize=13,loc = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### The plot  above shows the correlation coefficients of features VS saleprice, from high to low for all the features.\n",
    "And the p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets, which means the lower p-value is, the better correlation is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corrwith(df['SalePrice']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = df.corr().nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(df[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.fontsize=5\n",
    "plt.figure(figsize=(30,15))\n",
    "for i in range(20):\n",
    "    name = pearson_df.index[i]\n",
    "    plt.subplot(5,4,i+1)\n",
    "    plt.ylim(0,800000)\n",
    "    if df[name].unique().size>20:# nummeric features have more range\n",
    "        sns.regplot(x=name, y=\"SalePrice\", data=df[df[name]>0],scatter_kws={\"s\": 1.5,'alpha':0.7},line_kws={'color':'g','alpha':0.4})\n",
    "    else:\n",
    "#         sns.regplot(x=name, y=\"SalePrice\", data=df[df[name]>0],scatter_kws={\"s\": 1.5,'alpha':0.7},line_kws={'color':'g','alpha':0.4},x_estimator=np.mean)\n",
    "        sns.boxplot(x=name, y='SalePrice',data=df[df[name]>0],palette='hls',linewidth=0.7)\n",
    "    plt.title(name,y=0.85)\n",
    "    plt.xlabel('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer look at some of the numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GarageArea (Size of garage in square feet)\n",
    "- TotalBsmtSF (Total square feet of basement area)\n",
    "- YearBuilt ( Original Constrbbuction Data)\n",
    "- TrueYear (Engineered Feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GrLivArea (Size of garage in square feet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "plt.figure(figsize=(15,4))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 2]) \n",
    "plt.subplot(gs[0])\n",
    "plt.hist(df['GrLivArea'],bins=30)\n",
    "plt.xlabel('Above grade (ground) living area square feet',fontsize=14)\n",
    "plt.ylabel('Count',fontsize=14)\n",
    "\n",
    "plt.subplot(gs[1])\n",
    "\n",
    "sns.regplot(x='GrLivArea', y=\"SalePrice\",data=df,robust=True,scatter_kws={\"s\": 3},line_kws={'color':'g','alpha':0.4})\n",
    "plt.xlim([0,3500])\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('SalePrice',fontsize=14)\n",
    "plt.xlabel('Above grade (ground) living area square feet',fontsize=14)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram above shows the distribution of the GrLivArea. And the scatter plot with regression line shows that there is clear correlation between GrLivArea and SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['log_GrLivArea']= (df['GrLivArea']).apply(lambda x:np.log(x))\n",
    "# fig4, ax = plt.subplots(figsize=(5,5)) \n",
    "    \n",
    "# sns.scatterplot(data=df, y=df['SalePrice'], x=df['log_GrLivArea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  TotalBsmtSF (Total square feet of basement area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import gridspec\n",
    "plt.figure(figsize=(15,4))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 2]) \n",
    "plt.subplot(gs[0])\n",
    "plt.hist(df['TotalBsmtSF'],bins=30)\n",
    "plt.xlabel('Total square feet of basement area',fontsize=14)\n",
    "plt.ylabel('Count',fontsize=14)\n",
    "\n",
    "plt.subplot(gs[1])\n",
    "\n",
    "sns.regplot(x='TotalBsmtSF', y=\"SalePrice\",data=df,robust=True,scatter_kws={\"s\": 3},line_kws={'color':'g','alpha':0.4})\n",
    "plt.xlim([0,3500])\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('SalePrice',fontsize=14)\n",
    "plt.xlabel('Total square feet of basement area',fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram above shows the distribution of the TotalBsmtSF. And the scatter plot with regression line shows that there is clear correlation between TotalBsmtSF and SalePrice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YearBuilt (Original Construction Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(12,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(df.YearBuilt,bins=100)\n",
    "YearMean=df.groupby(['YearBuilt']).SalePrice.mean()\n",
    "YearSize=df.groupby(['YearBuilt']).SalePrice.count()\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Count',fontsize=14)\n",
    "plt.xlabel('Year Built',fontsize=14)\n",
    "plt.title('Distribution of Number of Houses Built Every Year',fontsize=16,y=0.9)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.scatter(YearMean.index,YearMean,s=YearSize*5,alpha=0.8)\n",
    "\n",
    "fit = np.polyfit(df.YearBuilt,df.SalePrice,deg=1)\n",
    "plt.plot(YearMean.index, fit[0] * YearMean.index + fit[1], color='green',alpha=0.6)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Sale Price',fontsize=14)\n",
    "plt.xlabel('YearBuilt',fontsize=14)\n",
    "plt.title('Correlation between YearBuilt and SalePrice',fontsize=16,y=0.9)\n",
    "plt.legend(['Regression Line','SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a general linear relationship between Year built and Sale price, but some what might seem to be old building are still going for above average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### TrueYear (Engineered Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(12,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(df.TrueYear,bins=100)\n",
    "YearMean=df.groupby(['TrueYear']).SalePrice.mean()\n",
    "YearSize=df.groupby(['TrueYear']).SalePrice.count()\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Count',fontsize=14)\n",
    "plt.xlabel('True Year',fontsize=14)\n",
    "plt.title('Distribution of Number of Houses True Age',fontsize=16,y=0.9)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.scatter(YearMean.index,YearMean,s=YearSize*5,alpha=0.8)\n",
    "# the regression line is created with all the original SalePrice, not the year-mean value\n",
    "fit = np.polyfit(df.TrueYear,df.SalePrice,deg=1)\n",
    "plt.plot(YearMean.index, fit[0] * YearMean.index + fit[1], color='green',alpha=0.6)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Sale Price',fontsize=14)\n",
    "plt.xlabel('TrueYear',fontsize=14)\n",
    "plt.title('Correlation between TrueYear and SalePrice',fontsize=16,y=0.9)\n",
    "plt.legend(['Regression Line','SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more of a linear relationship between TrueYear and SalesPrice which is expected.Because accounting for rennovation in the houses would explain why some old houses still went for high prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm= ['January','February','March','April','May','June','July','August','September','October','November','December']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1= plt.figure(figsize=(12,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(df.MoSold,bins=100)\n",
    "month=df.groupby(['MoSold']).SalePrice.count()\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xticks( [ 1,2,3, 4,5,6,7,8,9,10,11,12],lm, rotation=0)\n",
    "plt.ylabel('Count',fontsize=14)\n",
    "plt.xlabel('Months',fontsize=14)\n",
    "plt.title('Distribution of Number of Houses sales across months',fontsize=16,y=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "June and july over the years seem to be the highest selling months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between the categorical features and SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prepare the categorical features\n",
    "- rate the correlations of each features VS SalePrice and save in DataFrame\n",
    "- Sort the features by r_2 scores from high to low. Show the result in a table and a barplot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "############### prepare categorical features#################### \n",
    "# cat_df: categorical data from original df\n",
    "cat_df = df.drop(num_cols,axis=1).drop('SalePrice',axis=1)\n",
    "\n",
    "############### Rate the correlation with R_2 sccore############\n",
    "features_R = {}\n",
    "for feature in cat_df.columns.values.tolist():\n",
    "    model = ols('SalePrice ~'+feature, df).fit()\n",
    "    features_R[feature] = model.rsquared\n",
    "\n",
    "############### Sort the features by r_2 scores ################\n",
    "r_df = pd.DataFrame(list(features_R.items()),columns=['features','R_squared']).sort_values(by=['R_squared'],ascending=False)\n",
    "r_df.index = r_df['features']\n",
    "r_df = r_df.drop('features',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13,8))\n",
    "# plt.bar(r_df.index,r_df.R_squared)\n",
    "r_df.plot.bar(figsize=(13,8))\n",
    "plt.title('R_2 Scores Ranking of All Categorical Features VS SalePrice',fontsize=16)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(rotation=80,fontsize=14)\n",
    "plt.ylabel('r_2 score',fontsize=14)\n",
    "plt.xlabel('Features',fontsize=14)\n",
    "plt.legend(['r_2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a closer look at some top ranked features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neighborhood(Physical locations within Ames city limits)\n",
    "- ExterQual( Exterior material quality)\n",
    "- BsmtQual(Evaluates the height of the basement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = df.groupby('Neighborhood').count().Id.plot.bar()\n",
    "# ax = df.groupby('Neighborhood').count().reset_index('count').sort_values('count').Id.plot.bar()\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Count',fontsize=14)\n",
    "plt.xlabel('Neighborhood',fontsize=14)\n",
    "plt.title('Distribution of Number of Houses in Every Neighborhood',fontsize=16,y=0.9)\n",
    "\n",
    "fig2 = plt.figure(figsize=(12,5))\n",
    "ax = df.boxplot(column='SalePrice',by='Neighborhood',figsize=(11,5),rot=45)\n",
    "# ax = df.groupby('Neighborhood').SalePrice.plot.box()\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Sale Price',fontsize=14)\n",
    "plt.xlabel('Neighborhood',fontsize=14)\n",
    "plt.title('Correlation between Neighborhood and SalePrice',fontsize=16,y=0.9)\n",
    "plt.suptitle('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = df.groupby('ExterQual').count().Id.plot.bar()\n",
    "# ax = df.groupby('Neighborhood').count().reset_index('count').sort_values('count').Id.plot.bar()\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12,rotation=0)\n",
    "plt.ylabel('Count',fontsize=14)\n",
    "plt.xlabel('ExterQual (quality of the material on the exterior)',fontsize=14)\n",
    "plt.title('Distribution of Number of Houses in each category of ExterQual',fontsize=16,y=0.9)\n",
    "\n",
    "fig2 = plt.figure(figsize=(12,5))\n",
    "ax = df.boxplot(column='SalePrice',by='ExterQual',figsize=(11,5))\n",
    "# ax = df.groupby('Neighborhood').SalePrice.plot.box()\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Sale Price',fontsize=14)\n",
    "plt.xlabel('ExterQual (quality of the material on the exterior)',fontsize=14)\n",
    "plt.title('Correlation between ExterQual and SalePrice',fontsize=16,y=0.9)\n",
    "plt.suptitle('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, when it came to Fa (fair) quality of material on the exterior it seems to be compromisable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BsmtQual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = df.groupby('BsmtQual').count().Id.plot.bar()\n",
    "# ax = df.groupby('Neighborhood').count().reset_index('count').sort_values('count').Id.plot.bar()\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12,rotation=0)\n",
    "plt.ylabel('Count',fontsize=14)\n",
    "plt.xlabel('BsmtQual (Evaluates the height of the basement)',fontsize=14)\n",
    "plt.title('Distribution of Number of Houses in each category of BsmtQual',fontsize=16,y=0.9)\n",
    "\n",
    "fig2 = plt.figure(figsize=(12,5))\n",
    "ax = df.boxplot(column='SalePrice',by='BsmtQual',figsize=(11,5))\n",
    "# ax = df.groupby('Neighborhood').SalePrice.plot.box()\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Sale Price',fontsize=14)\n",
    "plt.xlabel('BsmtQual (Evaluates the height of the basement)',fontsize=14)\n",
    "plt.title('Correlation between BsmtQual and SalePrice',fontsize=16,y=0.9)\n",
    "plt.suptitle('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, it is obvious that houses in some BsmtQual have higher price, and some have lower price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High correlated features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering correlation(or r_squared)>0.05 as significant, the top numerical features and top categorical features are listed below, and we are going to use these useful_features for the following model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute values >0.05\n",
    "num_top_features = pearson_df[abs(pearson_df['correlation'])>0.05].index.values.tolist()\n",
    "cat_top_features = r_df[abs(r_df['R_squared'])>0.05].index.values.tolist()\n",
    "useful_features = num_top_features+cat_top_features\n",
    "print(len(num_top_features),'numerical features:\\n',num_top_features,'\\n')\n",
    "print(len(cat_top_features),'catigorical features:\\n',cat_top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# def preprocess_features(X):\n",
    "#     # Initialize new output DataFrame(empty)\n",
    "#     output = pd.DataFrame(index = X.index)\n",
    "    \n",
    "#     # Investigate each feature column for the data\n",
    "#     for col, col_data in X.iteritems():\n",
    "#         #print(col,col_data)\n",
    "\n",
    "#         # If data type is categorical, convert to dummy variables\n",
    "#         if col_data.dtype == object:\n",
    "#             col_data = pd.get_dummies(col_data, prefix = col)  \n",
    "        \n",
    "#         # Collect the revised columns\n",
    "#         output = output.join(col_data)\n",
    "    \n",
    "#     return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Keep only the useful features in X\n",
    "# df_featured = df[useful_features]\n",
    "# y = np.log(df.SalePrice)\n",
    "# X = df_featured.loc[:,df_featured.columns != 'SalePrice']\n",
    "\n",
    "# # X_all is df all numeric, and no \"SalePrice\"\n",
    "# X_all = preprocess_features(X)\n",
    "# X_all.info()\n",
    "\n",
    "# # Split the targets into training/testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_all, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer look at the target variable (SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "sns.distplot(df.SalePrice);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df_train['SalePrice'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SalePrice is not normal distributed\n",
    "- Have positive skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "sns.distplot(df['SalePrice'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, 'SalePrice' is not normally distributed. We can solve this with simple log transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transform saleprice\n",
    "df.SalePrice = np.log(df.SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram and normal probability plot\n",
    "sns.distplot(df.SalePrice, fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df.SalePrice, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[useful_features].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have established that 50 of my features are useful after the EDA, 25 categorical and 25 numeric stored in varaibale useful_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(useful_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datafram of useful feat in df2\n",
    "df2=df[useful_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat2=df2.select_dtypes(include='object')\n",
    "num2=df2.select_dtypes(exclude='object')\n",
    "print('We have :',cat2.shape[1],'categorical features with missing values')\n",
    "print('We have :',num2.shape[1],'numerical features with missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of the numeric variables, some would be treated as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical(df):\n",
    "    #function to get catergorical varaiables from my 'numeric' dataframe because some variables are numeric but categorical\n",
    "    cat_num=[]\n",
    "    for i in df.columns:\n",
    "        if df[i].unique().size<17:\n",
    "            cat_num.append(i)\n",
    "    return cat_num\n",
    "get_categorical(num2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'll begining with numerical features that are actually categorical, for example \"Month sold\", the values are from 1 to 12\n",
    "- Then the MSSubClass( The building class) making that a string too because it categorical\n",
    "- YrSold also since I want it to count as a categorical feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['MoSold'] = df2['MoSold'].astype(str)\n",
    "df2['MSSubClass'] = df2['MSSubClass'].apply(str)\n",
    "df2['YrSold'] =df2['YrSold'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_num= get_categorical(num2)\n",
    "df2[g_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.get_dummies(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set had some features engineered and some features droppedI'll do the same to the test set the plan is to end up with 50 features(before getting dummies) 25 categorical and 25 numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop columns with too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable containing dropped featues\n",
    "removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test= df_test.drop(columns= removed, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feature enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true age is since when last remodel else sice when built\n",
    "df_test['TrueYear'] = np.where(df_test['YearRemodAdd'] > 0, df_test['YearRemodAdd'], df_test['YearBuilt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature total area an addition of all the measurements of the building \n",
    "df_test['TotalArea'] =df_test['TotalBsmtSF'] + df_test['1stFlrSF'] + df_test['2ndFlrSF'] + df_test['GrLivArea'] +df_test['GarageArea']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final selection of features used in the training \n",
    "useful_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
